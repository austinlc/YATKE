{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Sensitivity Coefficients for the Parameters Entered Into BrosaYields Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "from statistics import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import sympy as sym\n",
    "from sympy.interactive import printing\n",
    "printing.init_printing(use_latex = 'mathjax')\n",
    "from IPython.display import display, Math, Latex\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.pyplot import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdata(dataID, dataFolders):\n",
    "    \n",
    "    for folderName in dataFolders:\n",
    "        print('Data Gathered from {}. '.format(folderName))\n",
    "        \n",
    "        folderPath = os.path.join('/home/austinlc/Documents/SensitivityData',folderName)\n",
    "\n",
    "        if not os.path.isdir(folderPath):\n",
    "            raise ValueError('Bad Path for folder named: {}'.format(folderPath))\n",
    "        \n",
    "        files = sorted(os.listdir(folderPath))\n",
    "        \n",
    "        for filename in files:\n",
    "            \n",
    "            file = os.path.join(folderPath,filename)\n",
    "            \n",
    "            working_file = open(file)\n",
    "            \n",
    "            line = working_file.readline()\n",
    "            if (line.find(\"#\")!=0):\n",
    "                sys.exit(\"ERROR: FIRST LINE OF OUTPUT FILE SHOULD CONTAIN '#'\")\n",
    "                return 0\n",
    "            \n",
    "            if dataID == \"RT\":\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: RT\n",
    "                except NameError: RT = {}\n",
    "                \n",
    "                RTfromFile = float(filename[filename.index('RT')+2:])/100\n",
    "                RT[filename] = RTfromFile\n",
    "                \n",
    "            elif dataID == \"alpha\":\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: alpha\n",
    "                except NameError: alpha = {}\n",
    "                \n",
    "                alphafromFile = float(filename[filename.index('ALPHA')+5:])/100\n",
    "                alpha[filename] = alphafromFile\n",
    "                \n",
    "            elif dataID == \"Al\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Al\n",
    "                except NameError: Al = {}\n",
    "                Al[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        Al[filename].append(int(tempdata1[0]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"Ah\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Ah\n",
    "                except NameError: Ah = {}\n",
    "                Ah[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        Ah[filename].append(int(tempdata1[0]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"A\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: A\n",
    "                except NameError: A = {}\n",
    "                A[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0):  break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    A[filename].append(int(tempdata1[0]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Zl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Zl\n",
    "                except NameError: Zl = {}\n",
    "                Zl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        Zl[filename].append(int(tempdata1[1]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"Zh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Zh\n",
    "                except NameError: Zh = {}\n",
    "                Zh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        Zh[filename].append(int(tempdata1[1]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Z\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Z\n",
    "                except NameError: Z = {}\n",
    "                Z[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    Z[filename].append(int(tempdata1[1]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Ul\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Ul\n",
    "                except NameError: Ul = {}\n",
    "                Ul[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        Ul[filename].append(float(tempdata1[2]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"Uh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Uh\n",
    "                except NameError: Uh = {}\n",
    "                Uh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        Uh[filename].append(float(tempdata1[2]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"TXE\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: TXE\n",
    "                except NameError: TXE = {}\n",
    "                TXE[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        Ul = (float(tempdata1[2]))\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        Uh = (float(tempdata1[2]))\n",
    "                        TXE[filename].append(Ul + Uh) #append after gathering the heavy frag excitation energy\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Jl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Jl\n",
    "                except NameError: Jl = {}\n",
    "                Jl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        Jl[filename].append(float(tempdata1[3]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"Jh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Jh\n",
    "                except NameError: Jh = {}\n",
    "                Jh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        Jh[filename].append(float(tempdata1[3]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"J\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: J\n",
    "                except NameError: J = {}\n",
    "                J[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    J[filename].append(float(tempdata1[3]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"KEl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: KEl\n",
    "                except NameError: KEl = {}\n",
    "                KEl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        KEl[filename].append(float(tempdata1[5]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"KEh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: KEh\n",
    "                except NameError: KEh = {}\n",
    "                KEh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        KEh[filename].append(float(tempdata1[5]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"TKE\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: TKE\n",
    "                except NameError: TKE = {}\n",
    "                TKE[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        KEl = (float(tempdata1[5]))\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        KEh = (float(tempdata1[5]))\n",
    "                        TKE[filename].append(KEl + KEh) #append after gathering the heavy frag\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"nnl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nnl\n",
    "                except NameError: nnl = {}\n",
    "                nnl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        nnl[filename].append(int(tempdata1[6]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"nnh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nnh\n",
    "                except NameError: nnh = {}\n",
    "                nnh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        nnh[filename].append(int(tempdata1[6]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"nnt\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nnt\n",
    "                except NameError: nnt = {}\n",
    "                nnt[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        nnl = (int(tempdata1[6]))\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        nnh = (int(tempdata1[6]))\n",
    "                        nnt[filename].append(nnl + nnh) #append after gathering the heavy frag excitation energy\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"ngl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ngl\n",
    "                except NameError: ngl = {}\n",
    "                ngl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        ngl[filename].append(int(tempdata1[7]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"ngh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ngh\n",
    "                except NameError: ngh = {}\n",
    "                ngh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        ngh[filename].append(int(tempdata1[7]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"ngt\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ngt\n",
    "                except NameError: ngt = {}\n",
    "                ngt[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        ngl = (int(tempdata1[7]))\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        ngh = (int(tempdata1[7]))\n",
    "                        ngt[filename].append(ngl + ngh) #append after gathering the heavy frag\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                        \n",
    "            elif dataID == \"nICl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nICl\n",
    "                except NameError: nICl = {}\n",
    "                nICl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        nICl[filename].append(int(tempdata1[8]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"nICh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nICh\n",
    "                except NameError: nICh = {}\n",
    "                nICh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        nICh[filename].append(int(tempdata1[8]))\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"nICt\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nICt\n",
    "                except NameError: nICt = {}\n",
    "                nICt[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        nICl = (int(tempdata1[8]))\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        nICh = (int(tempdata1[8]))\n",
    "                        nICt[filename].append(nICl + nICh) #append after gathering the heavy frag\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"pcml\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: pcml\n",
    "                except NameError: pcml = {}\n",
    "                pcml[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    line = working_file.readline()\n",
    "                    tempdata2 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        pcml[filename].append([tempdata2[0], tempdata2[1], tempdata2[2]])\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"pcmh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: pcmh\n",
    "                except NameError: pcmh = {}\n",
    "                pcmh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    line = working_file.readline()\n",
    "                    tempdata2 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        pcmh[filename].append([tempdata2[0], tempdata2[1], tempdata2[2]])\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"pcm\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: pcm\n",
    "                except NameError: pcm = {}\n",
    "                pcm[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    line = working_file.readline()\n",
    "                    tempdata2 = line.split()\n",
    "                    pcm[filename].append([tempdata2[0], tempdata2[1], tempdata2[2]])\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"plabl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: plabl\n",
    "                except NameError: plabl = {}\n",
    "                plabl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    line = working_file.readline()\n",
    "                    tempdata2 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        plabl[filename].append([tempdata2[3], tempdata2[4], tempdata2[5]])\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"plabh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: plabh\n",
    "                except NameError: plabh = {}\n",
    "                plabh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    line = working_file.readline()\n",
    "                    tempdata2 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        plabh[filename].append([tempdata2[3], tempdata2[4], tempdata2[5]])\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"plab\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: plab\n",
    "                except NameError: plab = {}\n",
    "                plab[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    line = working_file.readline()\n",
    "                    tempdata2 = line.split()\n",
    "                    plab[filename].append([tempdata2[3], tempdata2[4], tempdata2[5]])\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Encml\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Encml\n",
    "                except NameError: Encml = {}\n",
    "                Encml[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            Encml[filename].append(float(tempdata3[8*i + 3]))\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"Encmh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Encmh\n",
    "                except NameError: Encmh = {}\n",
    "                Encmh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            Encmh[filename].append(float(tempdata3[8*i + 3]))\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Encm\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Encm\n",
    "                except NameError: Encm = {}\n",
    "                Encm[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[6])):\n",
    "                        Encm[filename].append(float(tempdata3[8*i + 3]))\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Enlabl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Enlabl\n",
    "                except NameError: Enlabl = {}\n",
    "                Enlabl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            Enlabl[filename].append(float(tempdata3[8*i + 7]))\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"Enlabh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Enlabh\n",
    "                except NameError: Enlabh = {}\n",
    "                Enlabh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            Enlabh[filename].append(float(tempdata3[8*i + 7]))\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Enlab\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Enlab\n",
    "                except NameError: Enlab = {}\n",
    "                Enlab[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[6])):\n",
    "                        Enlab[filename].append(float(tempdata3[8*i + 7]))\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"Egcml\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Egcml\n",
    "                except NameError: Egcml = {}\n",
    "                Egcml[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[7])):\n",
    "                            Egcml[filename].append(float(tempdata3[8*i + 3]))\n",
    "            \n",
    "            elif dataID == \"Egcmh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Egcmh\n",
    "                except NameError: Egcmh = {}\n",
    "                Egcmh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[7])):\n",
    "                            Egcmh[filename].append(float(tempdata3[8*i + 3]))\n",
    "                \n",
    "            elif dataID == \"Egcm\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Egcm\n",
    "                except NameError: Egcm = {}\n",
    "                Egcm[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[7])):\n",
    "                        Egcm[filename].append(float(tempdata3[8*i + 3]))\n",
    "                \n",
    "            elif dataID == \"Eglabl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Eglabl\n",
    "                except NameError: Eglabl = {}\n",
    "                Eglabl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[7])):\n",
    "                            Eglabl[filename].append(float(tempdata3[8*i + 7]))\n",
    "            \n",
    "            elif dataID == \"Eglabh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Eglabh\n",
    "                except NameError: Eglabh = {}\n",
    "                Eglabh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[7])):\n",
    "                            Eglabh[filename].append(float(tempdata3[8*i + 7]))\n",
    "                \n",
    "            elif dataID == \"Eglab\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: Eglab\n",
    "                except NameError: Eglab = {}\n",
    "                Eglab[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0):\n",
    "                        line = working_file.readline()\n",
    "                        tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[7])):\n",
    "                        Eglab[filename].append(float(tempdata3[8*i + 7]))\n",
    "                \n",
    "            elif dataID == \"ndircml\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ndircml\n",
    "                except NameError: ndircml = {}\n",
    "                ndircml[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            ndircml[filename].append([tempdata3[8*i + 0],tempdata3[8*i + 1] ,tempdata3[8*i + 2]])\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"ndircmh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ndircmh\n",
    "                except NameError: ndircmh = {}\n",
    "                ndircmh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            ndircmh[filename].append([tempdata3[8*i + 0],tempdata3[8*i + 1] ,tempdata3[8*i + 2]])\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"ndircm\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ndircm\n",
    "                except NameError: ndircm = {}\n",
    "                ndircm[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[6])):\n",
    "                        ndircm[filename].append([tempdata3[8*i + 0],tempdata3[8*i + 1] ,tempdata3[8*i + 2]])\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"ndirlabl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ndirlabl\n",
    "                except NameError: ndirlabl = {}\n",
    "                ndirlabl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            ndirlabl[filename].append([tempdata3[8*i + 4],tempdata3[8*i + 5] ,tempdata3[8*i + 6]])\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "            \n",
    "            elif dataID == \"ndirlabh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ndirlabh\n",
    "                except NameError: ndirlabh = {}\n",
    "                ndirlabh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            ndirlabh[filename].append([tempdata3[8*i + 4],tempdata3[8*i + 5] ,tempdata3[8*i + 6]])\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"ndirlab\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: ndirlab\n",
    "                except NameError: ndirlab = {}\n",
    "                ndirlab[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[6])):\n",
    "                        ndirlab[filename].append([tempdata3[8*i + 4],tempdata3[8*i + 5] ,tempdata3[8*i + 6]])\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                \n",
    "            elif dataID == \"gdircml\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: gdircml\n",
    "                except NameError: gdircml = {}\n",
    "                gdircml[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            gdircml[filename].append([tempdata3[8*i + 0],tempdata3[8*i + 1] ,tempdata3[8*i + 2]])\n",
    "            \n",
    "            elif dataID == \"gdircmh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: gdircmh\n",
    "                except NameError: gdircmh = {}\n",
    "                gdircmh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            gdircmh[filename].append([tempdata3[8*i + 0],tempdata3[8*i + 1] ,tempdata3[8*i + 2]])\n",
    "                \n",
    "            elif dataID == \"gdircm\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: gdircm\n",
    "                except NameError: gdircm = {}\n",
    "                gdircm[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[6])):\n",
    "                        gdircm[filename].append([tempdata3[8*i + 0],tempdata3[8*i + 1] ,tempdata3[8*i + 2]])\n",
    "                \n",
    "            elif dataID == \"gdirlabl\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: gdirlabl\n",
    "                except NameError: gdirlabl = {}\n",
    "                gdirlabl[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 1): # LIGHT FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            gdirlabl[filename].append([tempdata3[8*i + 4],tempdata3[8*i + 5] ,tempdata3[8*i + 6]])\n",
    "            \n",
    "            elif dataID == \"gdirlabh\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: gdirlabh\n",
    "                except NameError: gdirlabh = {}\n",
    "                gdirlabh[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    if (nfragments % 2 == 0): # HEAVY FRAGMENT\n",
    "                        for i in range(int(tempdata1[6])):\n",
    "                            gdirlabh[filename].append([tempdata3[8*i + 4],tempdata3[8*i + 5] ,tempdata3[8*i + 6]])\n",
    "                \n",
    "            elif dataID == \"gdirlab\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: gdirlab\n",
    "                except NameError: gdirlab = {}\n",
    "                gdirlab[filename] = []\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    line = working_file.readline()\n",
    "                    tempdata3 = line.split()\n",
    "                    for i in range(int(tempdata1[6])):\n",
    "                        gdirlab[filename].append([tempdata3[8*i + 4],tempdata3[8*i + 5] ,tempdata3[8*i + 6]])\n",
    "\n",
    "            elif dataID == \"nevents\":\n",
    "                nfragments = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nevents\n",
    "                except NameError: nevents = {}\n",
    "                nevents[filename] = 0\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfragments+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                nevents[filename] = int(nfragments / 2)\n",
    "                \n",
    "            elif dataID == \"nfragments\":\n",
    "                nfrags = 0\n",
    "                # Existence Test - performed for EACH FILE\n",
    "                try: nfragments\n",
    "                except NameError: nfragments = {}\n",
    "                nfragments[filename] = 0\n",
    "                while True:\n",
    "                    line = working_file.readline()\n",
    "                    if (len(line)==0): break\n",
    "                    nfrags+=1\n",
    "                    tempdata1 = line.split()\n",
    "                    next(working_file)\n",
    "                    if (int(tempdata1[6]) != 0): next(working_file)\n",
    "                    if (int(tempdata1[7]) != 0): next(working_file)\n",
    "                nfragments[filename] = nfrags\n",
    "                \n",
    "            else:\n",
    "                print(\"ERROR: BAD DATA ID: {}, CHECK THAT ID IS CORRECT\".format(dataID))\n",
    "                return 0\n",
    "                           \n",
    "    # THE variable (dictionary) that is returned MUST BE THE SAME NAME AS THE STRING dataID\n",
    "    return eval(dataID)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotbib Version:  2.0.2\n"
     ]
    }
   ],
   "source": [
    "### rcParams are the default parameters for matplotlib\n",
    "import matplotlib as mpl\n",
    "\n",
    "print (\"Matplotbib Version: \", mpl.__version__)\n",
    "\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['font.family'] = 'Helvetica', 'serif'\n",
    "#mpl.rcParams['font.color'] = 'darkred'\n",
    "mpl.rcParams['font.weight'] = 'normal'\n",
    "\n",
    "mpl.rcParams['axes.labelsize'] = 18.\n",
    "mpl.rcParams['xtick.labelsize'] = 18.\n",
    "mpl.rcParams['ytick.labelsize'] = 18.\n",
    "mpl.rcParams['lines.linewidth'] = 2.\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'color'  : 'darkred',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18,\n",
    "        }\n",
    "\n",
    "mpl.rcParams['xtick.major.pad']='10'\n",
    "mpl.rcParams['ytick.major.pad']='10'\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'inferno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#-- Read JSON-formatted data file\n",
    "def readJSONDataFile (filename):\n",
    "    \n",
    "    with open(filename) as jsonFile:\n",
    "        jsonData = json.load(jsonFile)\n",
    "        \n",
    "    exp = list()\n",
    "    c=0\n",
    "    for item in jsonData.get(\"entries\"):\n",
    "        exp.append(item)\n",
    "        c=c+1\n",
    "        \n",
    "    return exp\n",
    "\n",
    "#-- Plot experimental data sets corresponding to a particular quantity\n",
    "def plotExperimentalData (quantity, **keyword_parameters):\n",
    "    if ('format' in keyword_parameters):\n",
    "        fmtplot = keyword_parameters['format']\n",
    "    else:\n",
    "        fmtplot = 'ko--'\n",
    "\n",
    "    checkAuthor = False\n",
    "    if ('author' in keyword_parameters):\n",
    "        author = keyword_parameters['author']\n",
    "        checkAuthor = True\n",
    "    for exp in experiments:\n",
    "        if (exp['quantity']==quantity):\n",
    "            if (checkAuthor):\n",
    "                if (author not in exp['authors']):\n",
    "                    continue\n",
    "            data = np.asarray(exp['data'])\n",
    "            if ('renorm' in exp):\n",
    "                coef = exp['renorm']\n",
    "            else:\n",
    "                coef = 1.0\n",
    "            x = data[:,0]\n",
    "            y = data[:,1]\n",
    "            if ('label' in keyword_parameters):\n",
    "                labelplot = keyword_parameters['label']\n",
    "            else:\n",
    "                labelplot = exp['label']\n",
    "            plt.plot(x,y*coef,fmtplot,alpha=0.5,label=labelplot)\n",
    "\n",
    "#-- Lists all experiments read in \n",
    "def listExperimentalData (experiments):\n",
    "    for exp in experiments:\n",
    "        print (\"{0:10s} |  {1}, {2}\".format(exp['quantity'], exp['authors'], exp['year']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YA                   |  F.-J. Hambsch, S. Oberstedt, P. Siegler, J. van Arle, R. Vogt, 1997\n",
      "YA                   |  C. Budtz-Joergensen and H.-H. Knitter, 1988\n",
      "YA                   |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "YA                   |  Sh. Zeynalov, F.-J.Hambsch, et al., 2011\n",
      "YZ                   |  Wahl, 1987\n",
      "TKEA                 |  Gook, 2014\n",
      "SIGTKEA              |  A. Gook et al., 2014\n",
      "Pnu                  |  P. Santi and M. Miller, 2008\n",
      "PFNS                 |  W. Mannhart, 1989\n",
      "PFNS2                |  W. Mannhart, 1989\n",
      "nubarA               |  Vorobyev et al., 2004\n",
      "EcmA                 |  Budtz-Jorgensen and Knitter, 1988\n",
      "nubarTKE             |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nubarTKE_A110        |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nubarTKE_A122        |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nubarTKE_A130        |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nubarTKE_A142        |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nLF                  |  A. Skarsvag and K. Bergheim, 1963\n",
      "nLF                  |  H. R. Bowman, S. G. Thompson, J. C. D. Milton, and W. J. Swiatecki, 1962\n",
      "nLF-A122             |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nLF-A96              |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nLF-A107             |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nLF-A116             |  A. Gook, F.-J. Hambsch, M. Vidali, 2014\n",
      "nn                   |  Pozzi et al., 2014\n",
      "Pnug                 |  A. Oberstedt, R. Billnert, F.-J. Hambsch, S. Oberstedt, et al., 2015\n",
      "PFGS1                |  R. Billnert, F.-J. Hambsch, A. Oberstedt, and S. Oberstedt, 2013\n",
      "PFGS2                |  R. Billnert, F.-J. Hambsch, A. Oberstedt, and S. Oberstedt, 2013\n",
      "multiplicityRatio    |  T. Wang et al., 2016\n",
      "PFGSvsA              |  A.Hotzel, P.Thirolf, Ch.Ender, D.Schwalm, M.Mutterer, P.Singer, M.Klemens, J.P.Theobald, M.Hesse, F.Goennenwein, H.v.d.Ploeg, 1996\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENTALDATAFILE = \"/home/austinlc/Documents/FromPTalou/expdata-98252sf.js\"\n",
    "\n",
    "experiments = readJSONDataFile (EXPERIMENTALDATAFILE)\n",
    "\n",
    "for exp in experiments:\n",
    "    print (\"{0:20s} |  {1}, {2}\".format(exp['quantity'], exp['authors'], exp['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n",
      "Data Gathered from w0. \n"
     ]
    }
   ],
   "source": [
    "# gather data - w0 \n",
    "\n",
    "w0 = [0.6, 0.72222222, 0.74666667, 0.77111111,  0.79555556, 0.82]\n",
    "\n",
    "avTXEw0 = []\n",
    "TXEw0 = getdata('TXE', ['w0'])\n",
    "for k,v in TXEw0.items(): avTXEw0.append(mean(v))\n",
    "del TXEw0 # save space and delete unnecessary data\n",
    "\n",
    "avTKEw0 = []\n",
    "TKEw0 = getdata('TKE', ['w0'])\n",
    "for k,v in TKEw0.items(): avTKEw0.append(mean(v))\n",
    "del TKEw0 # save space and delete unnecessary data\n",
    "\n",
    "avJw0 = []\n",
    "Jw0 = getdata('J', ['w0'])\n",
    "for k,v in Jw0.items(): avJw0.append(mean(v))\n",
    "del Jw0 # save space and delete unnecessary data\n",
    "\n",
    "nubarw0 = []\n",
    "nnw0 = getdata('nnt', ['w0'])\n",
    "for k,v in nnw0.items(): nubarw0.append(mean(v))\n",
    "\n",
    "nmom2w0 = []\n",
    "for k,v in nnw0.items(): \n",
    "    nmom2w0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2w0.append(mean(nmom2w0arr))\n",
    "\n",
    "nmom3w0 = []\n",
    "for k,v in nnw0.items(): \n",
    "    nmom3w0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3w0.append(mean(nmom3w0arr))\n",
    "del nnw0 # save space and delete unnecessary data\n",
    "\n",
    "Ngbarw0 = []\n",
    "ngw0 = getdata('ngt', ['w0'])\n",
    "for k,v in ngw0.items(): Ngbarw0.append(mean(v))\n",
    "\n",
    "gmom2w0 = []\n",
    "for k,v in ngw0.items(): \n",
    "    gmom2w0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2w0.append(mean(gmom2w0arr))\n",
    "\n",
    "gmom3w0 = []\n",
    "for k,v in ngw0.items(): \n",
    "    gmom3w0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3w0.append(mean(gmom3w0arr))\n",
    "del ngw0 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMw0 = []\n",
    "EnCMw0 = getdata('Encm', ['w0'])\n",
    "for k,v in EnCMw0.items(): avEnCMw0.append(mean(v))\n",
    "del EnCMw0 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMw0 = []\n",
    "EgCMw0 = getdata('Egcm', ['w0'])\n",
    "for k,v in EgCMw0.items(): avEgCMw0.append(mean(v))\n",
    "del EgCMw0 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabw0 = []\n",
    "EnLabw0 = getdata('Enlab', ['w0'])\n",
    "for k,v in EnLabw0.items(): avEnLabw0.append(mean(v))\n",
    "del EnLabw0 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabw0 = []\n",
    "EgLabw0 = getdata('Eglab', ['w0'])\n",
    "for k,v in EgLabw0.items(): avEgLabw0.append(mean(v))\n",
    "del EgLabw0 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n",
      "Data Gathered from w1. \n"
     ]
    }
   ],
   "source": [
    "# gather data - w1 \n",
    "\n",
    "w1 = np.linspace(0.15, 0.25, 10)\n",
    "\n",
    "avTXEw1 = []\n",
    "TXEw1 = getdata('TXE', ['w1'])\n",
    "for k,v in TXEw1.items(): avTXEw1.append(mean(v))\n",
    "del TXEw1 # save space and delete unnecessary data\n",
    "\n",
    "avTKEw1 = []\n",
    "TKEw1 = getdata('TKE', ['w1'])\n",
    "for k,v in TKEw1.items(): avTKEw1.append(mean(v))\n",
    "del TKEw1 # save space and delete unnecessary data\n",
    "\n",
    "avJw1 = []\n",
    "Jw1 = getdata('J', ['w1'])\n",
    "for k,v in Jw1.items(): avJw1.append(mean(v))\n",
    "del Jw1 # save space and delete unnecessary data\n",
    "\n",
    "nubarw1 = []\n",
    "nnw1 = getdata('nnt', ['w1'])\n",
    "for k,v in nnw1.items(): nubarw1.append(mean(v))\n",
    "\n",
    "nmom2w1 = []\n",
    "for k,v in nnw1.items(): \n",
    "    nmom2w1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2w1.append(mean(nmom2w1arr))\n",
    "\n",
    "nmom3w1 = []\n",
    "for k,v in nnw1.items(): \n",
    "    nmom3w1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3w1.append(mean(nmom3w1arr))\n",
    "del nnw1 # save space and delete unnecessary data\n",
    "\n",
    "Ngbarw1 = []\n",
    "ngw1 = getdata('ngt', ['w1'])\n",
    "for k,v in ngw1.items(): Ngbarw1.append(mean(v))\n",
    "\n",
    "gmom2w1 = []\n",
    "for k,v in ngw1.items(): \n",
    "    gmom2w1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2w1.append(mean(gmom2w1arr))\n",
    "\n",
    "gmom3w1 = []\n",
    "for k,v in ngw1.items(): \n",
    "    gmom3w1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3w1.append(mean(gmom3w1arr))\n",
    "del ngw1 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMw1 = []\n",
    "EnCMw1 = getdata('Encm', ['w1'])\n",
    "for k,v in EnCMw1.items(): avEnCMw1.append(mean(v))\n",
    "del EnCMw1 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMw1 = []\n",
    "EgCMw1 = getdata('Egcm', ['w1'])\n",
    "for k,v in EgCMw1.items(): avEgCMw1.append(mean(v))\n",
    "del EgCMw1 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabw1 = []\n",
    "EnLabw1 = getdata('Enlab', ['w1'])\n",
    "for k,v in EnLabw1.items(): avEnLabw1.append(mean(v))\n",
    "del EnLabw1 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabw1 = []\n",
    "EgLabw1 = getdata('Eglab', ['w1'])\n",
    "for k,v in EgLabw1.items(): avEgLabw1.append(mean(v))\n",
    "del EgLabw1 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n",
      "Data Gathered from w2. \n"
     ]
    }
   ],
   "source": [
    "# gather data - w2 \n",
    "\n",
    "w2 = np.linspace(0.001, 0.07, 10)\n",
    "\n",
    "avTXEw2 = []\n",
    "TXEw2 = getdata('TXE', ['w2'])\n",
    "for k,v in TXEw2.items(): avTXEw2.append(mean(v))\n",
    "del TXEw2 # save space and delete unnecessary data\n",
    "\n",
    "avTKEw2 = []\n",
    "TKEw2 = getdata('TKE', ['w2'])\n",
    "for k,v in TKEw2.items(): avTKEw2.append(mean(v))\n",
    "del TKEw2 # save space and delete unnecessary data\n",
    "\n",
    "avJw2 = []\n",
    "Jw2 = getdata('J', ['w2'])\n",
    "for k,v in Jw2.items(): avJw2.append(mean(v))\n",
    "del Jw2 # save space and delete unnecessary data\n",
    "\n",
    "nubarw2 = []\n",
    "nnw2 = getdata('nnt', ['w2'])\n",
    "for k,v in nnw2.items(): nubarw2.append(mean(v))\n",
    "\n",
    "nmom2w2 = []\n",
    "for k,v in nnw2.items(): \n",
    "    nmom2w2arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2w2.append(mean(nmom2w2arr))\n",
    "\n",
    "nmom3w2 = []\n",
    "for k,v in nnw2.items(): \n",
    "    nmom3w2arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3w2.append(mean(nmom3w2arr))\n",
    "del nnw2 # save space and delete unnecessary data\n",
    "\n",
    "Ngbarw2 = []\n",
    "ngw2 = getdata('ngt', ['w2'])\n",
    "for k,v in ngw2.items(): Ngbarw2.append(mean(v))\n",
    "\n",
    "gmom2w2 = []\n",
    "for k,v in ngw2.items(): \n",
    "    gmom2w2arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2w2.append(mean(gmom2w2arr))\n",
    "\n",
    "gmom3w2 = []\n",
    "for k,v in ngw2.items(): \n",
    "    gmom3w2arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3w2.append(mean(gmom3w2arr))\n",
    "del ngw2 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMw2 = []\n",
    "EnCMw2 = getdata('Encm', ['w2'])\n",
    "for k,v in EnCMw2.items(): avEnCMw2.append(mean(v))\n",
    "del EnCMw2 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMw2 = []\n",
    "EgCMw2 = getdata('Egcm', ['w2'])\n",
    "for k,v in EgCMw2.items(): avEgCMw2.append(mean(v))\n",
    "del EgCMw2 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabw2 = []\n",
    "EnLabw2 = getdata('Enlab', ['w2'])\n",
    "for k,v in EnLabw2.items(): avEnLabw2.append(mean(v))\n",
    "del EnLabw2 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabw2 = []\n",
    "EgLabw2 = getdata('Eglab', ['w2'])\n",
    "for k,v in EgLabw2.items(): avEgLabw2.append(mean(v))\n",
    "del EgLabw2 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Gathered from dmin0. \n",
      "Data Gathered from dmin0. \n",
      "Data Gathered from dmin0. \n",
      "Data Gathered from dmin0. \n",
      "Data Gathered from dmin0. \n",
      "Data Gathered from dmin0. \n",
      "Data Gathered from dmin0. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-728fb49bab8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mavEgCMdmin0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mEgCMdmin0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Egcm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'dmin0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEgCMdmin0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavEgCMdmin0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mEgCMdmin0\u001b[0m \u001b[0;31m# save space and delete unnecessary data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0ff536ef3643>\u001b[0m in \u001b[0;36mgetdata\u001b[0;34m(dataID, dataFolders)\u001b[0m\n\u001b[1;32m    731\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworking_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                     \u001b[0mnfragments\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m                     \u001b[0mtempdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m                     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# gather data - dmin0 \n",
    "\n",
    "dmin0 = np.linspace(10.,18.,10)\n",
    "#for some reason file 7 (index 6) did not work\n",
    "np.delete(dmin0, 6)\n",
    "\n",
    "avTXEdmin0 = []\n",
    "TXEdmin0 = getdata('TXE', ['dmin0'])\n",
    "for k,v in TXEdmin0.items(): avTXEdmin0.append(mean(v))\n",
    "del TXEdmin0 # save space and delete unnecessary data\n",
    "\n",
    "avTKEdmin0 = []\n",
    "TKEdmin0 = getdata('TKE', ['dmin0'])\n",
    "for k,v in TKEdmin0.items(): avTKEdmin0.append(mean(v))\n",
    "del TKEdmin0 # save space and delete unnecessary data\n",
    "\n",
    "avJdmin0 = []\n",
    "Jdmin0 = getdata('J', ['dmin0'])\n",
    "for k,v in Jdmin0.items(): avJdmin0.append(mean(v))\n",
    "del Jdmin0 # save space and delete unnecessary data\n",
    "\n",
    "nubardmin0 = []\n",
    "nndmin0 = getdata('nnt', ['dmin0'])\n",
    "for k,v in nndmin0.items(): nubardmin0.append(mean(v))\n",
    "\n",
    "nmom2dmin0 = []\n",
    "for k,v in nndmin0.items(): \n",
    "    nmom2dmin0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2dmin0.append(mean(nmom2dmin0arr))\n",
    "\n",
    "nmom3dmin0 = []\n",
    "for k,v in nndmin0.items(): \n",
    "    nmom3dmin0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3dmin0.append(mean(nmom3dmin0arr))\n",
    "del nndmin0 # save space and delete unnecessary data\n",
    "\n",
    "Ngbardmin0 = []\n",
    "ngdmin0 = getdata('ngt', ['dmin0'])\n",
    "for k,v in ngdmin0.items(): Ngbardmin0.append(mean(v))\n",
    "\n",
    "gmom2dmin0 = []\n",
    "for k,v in ngdmin0.items(): \n",
    "    gmom2dmin0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2dmin0.append(mean(gmom2dmin0arr))\n",
    "\n",
    "gmom3dmin0 = []\n",
    "for k,v in ngdmin0.items(): \n",
    "    gmom3dmin0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3dmin0.append(mean(gmom3dmin0arr))\n",
    "del ngdmin0 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMdmin0 = []\n",
    "EnCMdmin0 = getdata('Encm', ['dmin0'])\n",
    "for k,v in EnCMdmin0.items(): avEnCMdmin0.append(mean(v))\n",
    "del EnCMdmin0 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMdmin0 = []\n",
    "EgCMdmin0 = getdata('Egcm', ['dmin0'])\n",
    "for k,v in EgCMdmin0.items(): avEgCMdmin0.append(mean(v))\n",
    "del EgCMdmin0 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabdmin0 = []\n",
    "EnLabdmin0 = getdata('Enlab', ['dmin0'])\n",
    "for k,v in EnLabdmin0.items(): avEnLabdmin0.append(mean(v))\n",
    "del EnLabdmin0 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabdmin0 = []\n",
    "EgLabdmin0 = getdata('Eglab', ['dmin0'])\n",
    "for k,v in EgLabdmin0.items(): avEgLabdmin0.append(mean(v))\n",
    "del EgLabdmin0 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data - dmin1 \n",
    "\n",
    "dmin1 = np.linspace(6.,16.,10)\n",
    "\n",
    "avTXEdmin1 = []\n",
    "TXEdmin1 = getdata('TXE', ['dmin1'])\n",
    "for k,v in TXEdmin1.items(): avTXEdmin1.append(mean(v))\n",
    "del TXEdmin1 # save space and delete unnecessary data\n",
    "\n",
    "avTKEdmin1 = []\n",
    "TKEdmin1 = getdata('TKE', ['dmin1'])\n",
    "for k,v in TKEdmin1.items(): avTKEdmin1.append(mean(v))\n",
    "del TKEdmin1 # save space and delete unnecessary data\n",
    "\n",
    "avJdmin1 = []\n",
    "Jdmin1 = getdata('J', ['dmin1'])\n",
    "for k,v in Jdmin1.items(): avJdmin1.append(mean(v))\n",
    "del Jdmin1 # save space and delete unnecessary data\n",
    "\n",
    "nubardmin1 = []\n",
    "nndmin1 = getdata('nnt', ['dmin1'])\n",
    "for k,v in nndmin1.items(): nubardmin1.append(mean(v))\n",
    "\n",
    "nmom2dmin1 = []\n",
    "for k,v in nndmin1.items(): \n",
    "    nmom2dmin1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2dmin1.append(mean(nmom2dmin1arr))\n",
    "\n",
    "nmom3dmin1 = []\n",
    "for k,v in nndmin1.items(): \n",
    "    nmom3dmin1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3dmin1.append(mean(nmom3dmin1arr))\n",
    "del nndmin1 # save space and delete unnecessary data\n",
    "\n",
    "Ngbardmin1 = []\n",
    "ngdmin1 = getdata('ngt', ['dmin1'])\n",
    "for k,v in ngdmin1.items(): Ngbardmin1.append(mean(v))\n",
    "\n",
    "gmom2dmin1 = []\n",
    "for k,v in ngdmin1.items(): \n",
    "    gmom2dmin1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2dmin1.append(mean(gmom2dmin1arr))\n",
    "\n",
    "gmom3dmin1 = []\n",
    "for k,v in ngdmin1.items(): \n",
    "    gmom3dmin1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3dmin1.append(mean(gmom3dmin1arr))\n",
    "del ngdmin1 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMdmin1 = []\n",
    "EnCMdmin1 = getdata('Encm', ['dmin1'])\n",
    "for k,v in EnCMdmin1.items(): avEnCMdmin1.append(mean(v))\n",
    "del EnCMdmin1 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMdmin1 = []\n",
    "EgCMdmin1 = getdata('Egcm', ['dmin1'])\n",
    "for k,v in EgCMdmin1.items(): avEgCMdmin1.append(mean(v))\n",
    "del EgCMdmin1 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabdmin1 = []\n",
    "EnLabdmin1 = getdata('Enlab', ['dmin1'])\n",
    "for k,v in EnLabdmin1.items(): avEnLabdmin1.append(mean(v))\n",
    "del EnLabdmin1 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabdmin1 = []\n",
    "EgLabdmin1 = getdata('Eglab', ['dmin1'])\n",
    "for k,v in EgLabdmin1.items(): avEgLabdmin1.append(mean(v))\n",
    "del EgLabdmin1 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data - dmin2 \n",
    "\n",
    "dmin2 = np.linspace(10.,20.,10)\n",
    "\n",
    "avTXEdmin2 = []\n",
    "TXEdmin2 = getdata('TXE', ['dmin2'])\n",
    "for k,v in TXEdmin2.items(): avTXEdmin2.append(mean(v))\n",
    "del TXEdmin2 # save space and delete unnecessary data\n",
    "\n",
    "avTKEdmin2 = []\n",
    "TKEdmin2 = getdata('TKE', ['dmin2'])\n",
    "for k,v in TKEdmin2.items(): avTKEdmin2.append(mean(v))\n",
    "del TKEdmin2 # save space and delete unnecessary data\n",
    "\n",
    "avJdmin2 = []\n",
    "Jdmin2 = getdata('J', ['dmin2'])\n",
    "for k,v in Jdmin2.items(): avJdmin2.append(mean(v))\n",
    "del Jdmin2 # save space and delete unnecessary data\n",
    "\n",
    "nubardmin2 = []\n",
    "nndmin2 = getdata('nnt', ['dmin2'])\n",
    "for k,v in nndmin2.items(): nubardmin2.append(mean(v))\n",
    "\n",
    "nmom2dmin2 = []\n",
    "for k,v in nndmin2.items(): \n",
    "    nmom2dmin2arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2dmin2.append(mean(nmom2dmin2arr))\n",
    "\n",
    "nmom3dmin2 = []\n",
    "for k,v in nndmin2.items(): \n",
    "    nmom3dmin2arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3dmin2.append(mean(nmom3dmin2arr))\n",
    "del nndmin2 # save space and delete unnecessary data\n",
    "\n",
    "Ngbardmin2 = []\n",
    "ngdmin2 = getdata('ngt', ['dmin2'])\n",
    "for k,v in ngdmin2.items(): Ngbardmin2.append(mean(v))\n",
    "\n",
    "gmom2dmin2 = []\n",
    "for k,v in ngdmin2.items(): \n",
    "    gmom2dmin2arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2dmin2.append(mean(gmom2dmin2arr))\n",
    "\n",
    "gmom3dmin2 = []\n",
    "for k,v in ngdmin2.items(): \n",
    "    gmom3dmin2arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3dmin2.append(mean(gmom3dmin2arr))\n",
    "del ngdmin2 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMdmin2 = []\n",
    "EnCMdmin2 = getdata('Encm', ['dmin2'])\n",
    "for k,v in EnCMdmin2.items(): avEnCMdmin2.append(mean(v))\n",
    "del EnCMdmin2 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMdmin2 = []\n",
    "EgCMdmin2 = getdata('Egcm', ['dmin2'])\n",
    "for k,v in EgCMdmin2.items(): avEgCMdmin2.append(mean(v))\n",
    "del EgCMdmin2 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabdmin2 = []\n",
    "EnLabdmin2 = getdata('Enlab', ['dmin2'])\n",
    "for k,v in EnLabdmin2.items(): avEnLabdmin2.append(mean(v))\n",
    "del EnLabdmin2 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabdmin2 = []\n",
    "EgLabdmin2 = getdata('Eglab', ['dmin2'])\n",
    "for k,v in EgLabdmin2.items(): avEgLabdmin2.append(mean(v))\n",
    "del EgLabdmin2 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data - dmax0 \n",
    "\n",
    "dmax0 = np.linspace(15.,24.,10)\n",
    "np.delete(dmax0, 7)\n",
    "\n",
    "avTXEdmax0 = []\n",
    "TXEdmax0 = getdata('TXE', ['dmax0'])\n",
    "for k,v in TXEdmax0.items(): avTXEdmax0.append(mean(v))\n",
    "del TXEdmax0 # save space and delete unnecessary data\n",
    "\n",
    "avTKEdmax0 = []\n",
    "TKEdmax0 = getdata('TKE', ['dmax0'])\n",
    "for k,v in TKEdmax0.items(): avTKEdmax0.append(mean(v))\n",
    "del TKEdmax0 # save space and delete unnecessary data\n",
    "\n",
    "avJdmax0 = []\n",
    "Jdmax0 = getdata('J', ['dmax0'])\n",
    "for k,v in Jdmax0.items(): avJdmax0.append(mean(v))\n",
    "del Jdmax0 # save space and delete unnecessary data\n",
    "\n",
    "nubardmax0 = []\n",
    "nndmax0 = getdata('nnt', ['dmax0'])\n",
    "for k,v in nndmax0.items(): nubardmax0.append(mean(v))\n",
    "\n",
    "nmom2dmax0 = []\n",
    "for k,v in nndmax0.items(): \n",
    "    nmom2dmax0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2dmax0.append(mean(nmom2dmax0arr))\n",
    "\n",
    "nmom3dmax0 = []\n",
    "for k,v in nndmax0.items(): \n",
    "    nmom3dmax0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3dmax0.append(mean(nmom3dmax0arr))\n",
    "del nndmax0 # save space and delete unnecessary data\n",
    "\n",
    "Ngbardmax0 = []\n",
    "ngdmax0 = getdata('ngt', ['dmax0'])\n",
    "for k,v in ngdmax0.items(): Ngbardmax0.append(mean(v))\n",
    "\n",
    "gmom2dmax0 = []\n",
    "for k,v in ngdmax0.items(): \n",
    "    gmom2dmax0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2dmax0.append(mean(gmom2dmax0arr))\n",
    "\n",
    "gmom3dmax0 = []\n",
    "for k,v in ngdmax0.items(): \n",
    "    gmom3dmax0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3dmax0.append(mean(gmom3dmax0arr))\n",
    "del ngdmax0 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMdmax0 = []\n",
    "EnCMdmax0 = getdata('Encm', ['dmax0'])\n",
    "for k,v in EnCMdmax0.items(): avEnCMdmax0.append(mean(v))\n",
    "del EnCMdmax0 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMdmax0 = []\n",
    "EgCMdmax0 = getdata('Egcm', ['dmax0'])\n",
    "for k,v in EgCMdmax0.items(): avEgCMdmax0.append(mean(v))\n",
    "del EgCMdmax0 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabdmax0 = []\n",
    "EnLabdmax0 = getdata('Enlab', ['dmax0'])\n",
    "for k,v in EnLabdmax0.items(): avEnLabdmax0.append(mean(v))\n",
    "del EnLabdmax0 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabdmax0 = []\n",
    "EgLabdmax0 = getdata('Eglab', ['dmax0'])\n",
    "for k,v in EgLabdmax0.items(): avEgLabdmax0.append(mean(v))\n",
    "del EgLabdmax0 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data - dmax1 \n",
    "\n",
    "dmax1 = np.linspace(12.,21.,10)\n",
    "np.delete(dmax1, 0)\n",
    "\n",
    "avTXEdmax1 = []\n",
    "TXEdmax1 = getdata('TXE', ['dmax1'])\n",
    "for k,v in TXEdmax1.items(): avTXEdmax1.append(mean(v))\n",
    "del TXEdmax1 # save space and delete unnecessary data\n",
    "\n",
    "avTKEdmax1 = []\n",
    "TKEdmax1 = getdata('TKE', ['dmax1'])\n",
    "for k,v in TKEdmax1.items(): avTKEdmax1.append(mean(v))\n",
    "del TKEdmax1 # save space and delete unnecessary data\n",
    "\n",
    "avJdmax1 = []\n",
    "Jdmax1 = getdata('J', ['dmax1'])\n",
    "for k,v in Jdmax1.items(): avJdmax1.append(mean(v))\n",
    "del Jdmax1 # save space and delete unnecessary data\n",
    "\n",
    "nubardmax1 = []\n",
    "nndmax1 = getdata('nnt', ['dmax1'])\n",
    "for k,v in nndmax1.items(): nubardmax1.append(mean(v))\n",
    "\n",
    "nmom2dmax1 = []\n",
    "for k,v in nndmax1.items(): \n",
    "    nmom2dmax1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2dmax1.append(mean(nmom2dmax1arr))\n",
    "\n",
    "nmom3dmax1 = []\n",
    "for k,v in nndmax1.items(): \n",
    "    nmom3dmax1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3dmax1.append(mean(nmom3dmax1arr))\n",
    "del nndmax1 # save space and delete unnecessary data\n",
    "\n",
    "Ngbardmax1 = []\n",
    "ngdmax1 = getdata('ngt', ['dmax1'])\n",
    "for k,v in ngdmax1.items(): Ngbardmax1.append(mean(v))\n",
    "\n",
    "gmom2dmax1 = []\n",
    "for k,v in ngdmax1.items(): \n",
    "    gmom2dmax1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2dmax1.append(mean(gmom2dmax1arr))\n",
    "\n",
    "gmom3dmax1 = []\n",
    "for k,v in ngdmax1.items(): \n",
    "    gmom3dmax1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3dmax1.append(mean(gmom3dmax1arr))\n",
    "del ngdmax1 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMdmax1 = []\n",
    "EnCMdmax1 = getdata('Encm', ['dmax1'])\n",
    "for k,v in EnCMdmax1.items(): avEnCMdmax1.append(mean(v))\n",
    "del EnCMdmax1 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMdmax1 = []\n",
    "EgCMdmax1 = getdata('Egcm', ['dmax1'])\n",
    "for k,v in EgCMdmax1.items(): avEgCMdmax1.append(mean(v))\n",
    "del EgCMdmax1 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabdmax1 = []\n",
    "EnLabdmax1 = getdata('Enlab', ['dmax1'])\n",
    "for k,v in EnLabdmax1.items(): avEnLabdmax1.append(mean(v))\n",
    "del EnLabdmax1 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabdmax1 = []\n",
    "EgLabdmax1 = getdata('Eglab', ['dmax1'])\n",
    "for k,v in EgLabdmax1.items(): avEgLabdmax1.append(mean(v))\n",
    "del EgLabdmax1 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data - dmax2 \n",
    "\n",
    "dmax2 = np.linspace(12.,21.,10)\n",
    "\n",
    "avTXEdmax2 = []\n",
    "TXEdmax2 = getdata('TXE', ['dmax2'])\n",
    "for k,v in TXEdmax2.items(): avTXEdmax2.append(mean(v))\n",
    "del TXEdmax2 # save space and delete unnecessary data\n",
    "\n",
    "avTKEdmax2 = []\n",
    "TKEdmax2 = getdata('TKE', ['dmax2'])\n",
    "for k,v in TKEdmax2.items(): avTKEdmax2.append(mean(v))\n",
    "del TKEdmax2 # save space and delete unnecessary data\n",
    "\n",
    "avJdmax2 = []\n",
    "Jdmax2 = getdata('J', ['dmax2'])\n",
    "for k,v in Jdmax2.items(): avJdmax2.append(mean(v))\n",
    "del Jdmax2 # save space and delete unnecessary data\n",
    "\n",
    "nubardmax2 = []\n",
    "nndmax2 = getdata('nnt', ['dmax2'])\n",
    "for k,v in nndmax2.items(): nubardmax2.append(mean(v))\n",
    "\n",
    "nmom2dmax2 = []\n",
    "for k,v in nndmax2.items(): \n",
    "    nmom2dmax2arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2dmax2.append(mean(nmom2dmax2arr))\n",
    "\n",
    "nmom3dmax2 = []\n",
    "for k,v in nndmax2.items(): \n",
    "    nmom3dmax2arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3dmax2.append(mean(nmom3dmax2arr))\n",
    "del nndmax2 # save space and delete unnecessary data\n",
    "\n",
    "Ngbardmax2 = []\n",
    "ngdmax2 = getdata('ngt', ['dmax2'])\n",
    "for k,v in ngdmax2.items(): Ngbardmax2.append(mean(v))\n",
    "\n",
    "gmom2dmax2 = []\n",
    "for k,v in ngdmax2.items(): \n",
    "    gmom2dmax2arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2dmax2.append(mean(gmom2dmax2arr))\n",
    "\n",
    "gmom3dmax2 = []\n",
    "for k,v in ngdmax2.items(): \n",
    "    gmom3dmax2arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3dmax2.append(mean(gmom3dmax2arr))\n",
    "del ngdmax2 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMdmax2 = []\n",
    "EnCMdmax2 = getdata('Encm', ['dmax2'])\n",
    "for k,v in EnCMdmax2.items(): avEnCMdmax2.append(mean(v))\n",
    "del EnCMdmax2 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMdmax2 = []\n",
    "EgCMdmax2 = getdata('Egcm', ['dmax2'])\n",
    "for k,v in EgCMdmax2.items(): avEgCMdmax2.append(mean(v))\n",
    "del EgCMdmax2 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabdmax2 = []\n",
    "EnLabdmax2 = getdata('Enlab', ['dmax2'])\n",
    "for k,v in EnLabdmax2.items(): avEnLabdmax2.append(mean(v))\n",
    "del EnLabdmax2 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabdmax2 = []\n",
    "EgLabdmax2 = getdata('Eglab', ['dmax2'])\n",
    "for k,v in EgLabdmax2.items(): avEgLabdmax2.append(mean(v))\n",
    "del EgLabdmax2 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data - ddec0 \n",
    "\n",
    "ddec0 = np.linspace(0.2,0.8,10)\n",
    "\n",
    "avTXEddec0 = []\n",
    "TXEddec0 = getdata('TXE', ['ddec0'])\n",
    "for k,v in TXEddec0.items(): avTXEddec0.append(mean(v))\n",
    "del TXEddec0 # save space and delete unnecessary data\n",
    "\n",
    "avTKEddec0 = []\n",
    "TKEddec0 = getdata('TKE', ['ddec0'])\n",
    "for k,v in TKEddec0.items(): avTKEddec0.append(mean(v))\n",
    "del TKEddec0 # save space and delete unnecessary data\n",
    "\n",
    "avJddec0 = []\n",
    "Jddec0 = getdata('J', ['ddec0'])\n",
    "for k,v in Jddec0.items(): avJddec0.append(mean(v))\n",
    "del Jddec0 # save space and delete unnecessary data\n",
    "\n",
    "nubarddec0 = []\n",
    "nnddec0 = getdata('nnt', ['ddec0'])\n",
    "for k,v in nnddec0.items(): nubarddec0.append(mean(v))\n",
    "\n",
    "nmom2ddec0 = []\n",
    "for k,v in nnddec0.items(): \n",
    "    nmom2ddec0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2ddec0.append(mean(nmom2ddec0arr))\n",
    "\n",
    "nmom3ddec0 = []\n",
    "for k,v in nnddec0.items(): \n",
    "    nmom3ddec0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3ddec0.append(mean(nmom3ddec0arr))\n",
    "del nnddec0 # save space and delete unnecessary data\n",
    "\n",
    "Ngbarddec0 = []\n",
    "ngddec0 = getdata('ngt', ['ddec0'])\n",
    "for k,v in ngddec0.items(): Ngbarddec0.append(mean(v))\n",
    "\n",
    "gmom2ddec0 = []\n",
    "for k,v in ngddec0.items(): \n",
    "    gmom2ddec0arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2ddec0.append(mean(gmom2ddec0arr))\n",
    "\n",
    "gmom3ddec0 = []\n",
    "for k,v in ngddec0.items(): \n",
    "    gmom3ddec0arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3ddec0.append(mean(gmom3ddec0arr))\n",
    "del ngddec0 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMddec0 = []\n",
    "EnCMddec0 = getdata('Encm', ['ddec0'])\n",
    "for k,v in EnCMddec0.items(): avEnCMddec0.append(mean(v))\n",
    "del EnCMddec0 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMddec0 = []\n",
    "EgCMddec0 = getdata('Egcm', ['ddec0'])\n",
    "for k,v in EgCMddec0.items(): avEgCMddec0.append(mean(v))\n",
    "del EgCMddec0 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabddec0 = []\n",
    "EnLabddec0 = getdata('Enlab', ['ddec0'])\n",
    "for k,v in EnLabddec0.items(): avEnLabddec0.append(mean(v))\n",
    "del EnLabddec0 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabddec0 = []\n",
    "EgLabddec0 = getdata('Eglab', ['ddec0'])\n",
    "for k,v in EgLabddec0.items(): avEgLabddec0.append(mean(v))\n",
    "del EgLabddec0 # save space and delete unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data - ddec1 \n",
    "\n",
    "ddec1 = np.linspace(0.05,0.3,10)\n",
    "\n",
    "avTXEddec1 = []\n",
    "TXEddec1 = getdata('TXE', ['ddec1'])\n",
    "for k,v in TXEddec1.items(): avTXEddec1.append(mean(v))\n",
    "del TXEddec1 # save space and delete unnecessary data\n",
    "\n",
    "avTKEddec1 = []\n",
    "TKEddec1 = getdata('TKE', ['ddec1'])\n",
    "for k,v in TKEddec1.items(): avTKEddec1.append(mean(v))\n",
    "del TKEddec1 # save space and delete unnecessary data\n",
    "\n",
    "avJddec1 = []\n",
    "Jddec1 = getdata('J', ['ddec1'])\n",
    "for k,v in Jddec1.items(): avJddec1.append(mean(v))\n",
    "del Jddec1 # save space and delete unnecessary data\n",
    "\n",
    "nubarddec1 = []\n",
    "nnddec1 = getdata('nnt', ['ddec1'])\n",
    "for k,v in nnddec1.items(): nubarddec1.append(mean(v))\n",
    "\n",
    "nmom2ddec1 = []\n",
    "for k,v in nnddec1.items(): \n",
    "    nmom2ddec1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    nmom2ddec1.append(mean(nmom2ddec1arr))\n",
    "\n",
    "nmom3ddec1 = []\n",
    "for k,v in nnddec1.items(): \n",
    "    nmom3ddec1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    nmom3ddec1.append(mean(nmom3ddec1arr))\n",
    "del nnddec1 # save space and delete unnecessary data\n",
    "\n",
    "Ngbarddec1 = []\n",
    "ngddec1 = getdata('ngt', ['ddec1'])\n",
    "for k,v in ngddec1.items(): Ngbarddec1.append(mean(v))\n",
    "\n",
    "gmom2ddec1 = []\n",
    "for k,v in ngddec1.items(): \n",
    "    gmom2ddec1arr = [v[i]*(v[i]-1) for i in range(len(v))]\n",
    "    gmom2ddec1.append(mean(gmom2ddec1arr))\n",
    "\n",
    "gmom3ddec1 = []\n",
    "for k,v in ngddec1.items(): \n",
    "    gmom3ddec1arr = [v[i]*(v[i]-1)*(v[i]-2) for i in range(len(v))]\n",
    "    gmom3ddec1.append(mean(gmom3ddec1arr))\n",
    "del ngddec1 # save space and delete unnecessary data\n",
    "\n",
    "avEnCMddec1 = []\n",
    "EnCMddec1 = getdata('Encm', ['ddec1'])\n",
    "for k,v in EnCMddec1.items(): avEnCMddec1.append(mean(v))\n",
    "del EnCMddec1 # save space and delete unnecessary data\n",
    "\n",
    "avEgCMddec1 = []\n",
    "EgCMddec1 = getdata('Egcm', ['ddec1'])\n",
    "for k,v in EgCMddec1.items(): avEgCMddec1.append(mean(v))\n",
    "del EgCMddec1 # save space and delete unnecessary data\n",
    "\n",
    "avEnLabddec1 = []\n",
    "EnLabddec1 = getdata('Enlab', ['ddec1'])\n",
    "for k,v in EnLabddec1.items(): avEnLabddec1.append(mean(v))\n",
    "del EnLabddec1 # save space and delete unnecessary data\n",
    "\n",
    "avEgLabddec1 = []\n",
    "EgLabddec1 = getdata('Eglab', ['ddec1'])\n",
    "for k,v in EgLabddec1.items(): avEgLabddec1.append(mean(v))\n",
    "del EgLabddec1 # save space and delete unnecessary data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
